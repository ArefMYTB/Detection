{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m54UziKsgdka"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyFGn9kHd3RM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EtvB8QgghSq"
   },
   "source": [
    "# Transformation\n",
    "\n",
    "We convert the image into a PyTorch tensor and scale pixel values to the range [0.0, 1.0] by dividing by 255.\n",
    "\n",
    "We then normalize the tensor image with mean and standard deviation per channel (Red, Green, Blue) using the formula:\n",
    "\n",
    "$$\n",
    "\\text{normalized_pixel} = \\frac{(\\text{pixel} - \\mu)}{\\sigma}\n",
    "$$\n",
    "with $$\\mu = \\sigma = 0.5$$\n",
    "This scales the pixel values from $[0, 1]$ to $[-1, 1]$.\n",
    "\n",
    "Why Normalize?\n",
    "Neural networks train faster and more stably when input data is standardized (mean ≈ 0, std ≈ 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TIFygaSfzQu"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw5un1ouDqJU"
   },
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-5Kgh4hBgZKK",
    "outputId": "6853ec9c-840c-466f-95e7-d250ec617213"
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l0R8LBWI1pL"
   },
   "source": [
    "**Labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y__090bUFOYf"
   },
   "outputs": [],
   "source": [
    "class_names = ['plain', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPim3tTcJP-y"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldmO9Z-BMELx"
   },
   "source": [
    "**Convolution Output Size Formula**\n",
    "\n",
    "Given:\n",
    "- **N** = input size (height or width)\n",
    "- **K** = kernel size\n",
    "- **P** = padding\n",
    "- **S** = stride\n",
    "\n",
    "The output size (per dimension) is:\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left\\lfloor \\frac{N + 2P - K}{S} \\right\\rfloor + 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "This formula calculates the spatial dimension of the output after applying a convolution. Padding (`P`) controls how much the input is extended at the borders. Using `P = (K - 1) // 2` and `S = 1` helps preserve the input size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZuLb9aXJOQS"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # Given the image shape of (3, 32, 32)\n",
    "    self.conv1 = nn.Conv2d(3, 12, 5)  # -> (12, 28, 28)\n",
    "    self.pool = nn.MaxPool2d(2, 2)  # -> (12, 14, 14)\n",
    "    self.conv2 = nn.Conv2d(12, 24, 5)  # -> (24, 10, 10) -> then another pooling -> (24, 5, 5) -> then flatten -> (24 * 5 * 5)\n",
    "\n",
    "    self.fc1 = nn.Linear(24*5*5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = torch.flatten(x, 1)\n",
    "\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YH2iIktQFKL"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpS8wm8gTbl0"
   },
   "outputs": [],
   "source": [
    "net = NeuralNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HQjg_IWRgzd"
   },
   "source": [
    "**SGD and Momentum (Optimizer)**\n",
    "\n",
    "**SGD (Stochastic Gradient Descent)** updates model parameters using gradients to minimize the loss:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla L(\\theta)\n",
    "$$\n",
    "\n",
    "- η (learning rate), e.g., `lr=0.001`\n",
    "\n",
    "---\n",
    "\n",
    "**Momentum** helps speed up SGD and smooth updates by remembering past gradients:\n",
    "\n",
    "$$\n",
    "v_t = \\mu \\cdot v_{t-1} - \\eta \\cdot \\nabla L(\\theta)\n",
    "$$\n",
    "$$\n",
    "\\theta = \\theta + v_t\n",
    "$$\n",
    "\n",
    "- μ (momentum factor), e.g., `momentum=0.9`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xu9yrdM8QEnN"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6U-2QwLTdsH",
    "outputId": "6b92049f-bb21-4cb9-ff2d-18adc2ae13f2"
   },
   "outputs": [],
   "source": [
    "epoch_num = 20\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1f2ed0480aab4e92aa9d057daa169df3",
      "cdc71525bc584a3c857c942068182fdd",
      "54fb34a2b65748ba804c9af2ce24dafc",
      "b5e59bc0df6b47b98145c70aaa7a3e87",
      "d7533f6825a54f70942a5bb01dbe2d71",
      "e6c87a9f880244e4beaccf91f4efc70e",
      "1e85306f036941dcbfcafc007fc74378",
      "6fdf72fe375140998324c5656221c62c",
      "1ad7269bfb0c46a6a8e76fa4112109a9",
      "cbc966a890c44d3faff94ca758e708f8",
      "34c74de43bf545d59d783d314851e95d",
      "0fc50ccc0f6d49ea8f55b7f934b77334",
      "aa7a78923ea440c99f4e6727fd4be334",
      "e1c47a574e7f4a7aab3714c729a2a520",
      "9ed45c1a7b764a898de343227f700eed",
      "4f50e5456d5246c4934154d45cc453cc",
      "a0a7c0a5926a4b2baeaca776d99135e9",
      "8aaba0ccd30b47d5b4aca64da1be23cd",
      "5481ab32fcec4dd18ee1bc2fa920f24f",
      "88d0b5c6915248369ce90c7edc9db404",
      "1f24d99685604612950e30386293a18a",
      "d8fe3fefbe0b40049a7b8857f543e117",
      "1a406f375176419b8db822a64b66a98e",
      "b26cff7cbd624d6cbb0e02abd6c2c8cd",
      "68ac4796a3e6411882659e3969f61aed",
      "1b61c934e97047b59e707ee0fdfc59e3",
      "830b631bafb148d98138646c518ecfb2",
      "67d5cfed314d4016b27cfa1ad4843155",
      "c7108c78b4a649af927dc1a3918cb223",
      "714ccaec0f5d42c38352fa82b18797df",
      "08a562f2ce764bfd912cedf077433735",
      "1fc5dddc131e4d05a8e023c7ca5aa8ba",
      "cdf0dfb0f9c94b799174a3dc94bb07c8",
      "237ebc26a61248bdaf841b2d80c3437d",
      "1d31bb0500c74ba2a0c84e0286ef4d45",
      "56d3fd4fb1454bb4a1f99ab37018b2cf",
      "e1b08ea4a7054e21984dbdcb94f19a14",
      "90f23f2215e84f1d90a103c6e941f08e",
      "caa89f1943de49ab8a7a46e14909b2ad",
      "a6c7558a50684f5b8f172a6a7133c6f1",
      "23ad24b95c3d408585aa4cbb642c5911",
      "41a1226ddd7b4cffb0abffdf0adef650",
      "aa452d483ebb447b92038e5a2deaa41a",
      "ec56160a432a4101a7e47fb264f258c3",
      "241a7c90cde34608a0ca7fba2e59e9d6",
      "f2682aeca6c846bf9b7bf7a3e9de6ea1",
      "529a5bd8869841bf966e14e400b5a5b6",
      "e2e4beea6a61467795baa21e59afd706",
      "35f05dddb214457e90ff26d80024e0ae",
      "6db4fb45ae724a9085df1001b8f1c419",
      "7cbcf092236f4a459c39655fe0c32a2c",
      "a24c820ea3e247efaa2d05eadb69a2ba",
      "01c2fab20cdf414493d4f7e17162ffec",
      "409058a0690448ad935495c39939dc4b",
      "a630956b9a4f4ffab3cf036d102de1be",
      "259a72785b4b4306a1fbdd30a22ab692",
      "0e97970fddea43ceb4fb317c7f3a47a3",
      "351dcaeb5ca74a1c8dd1ff369196a4cf",
      "6ba015d6d6794c8c869cbfcb52047d1f",
      "d48055556e354ef58c03d162be516c5c",
      "f9b61f2a93604bc695158e677852019c",
      "be7ead2ba5544f6fb9bb45c23d656742",
      "1f57a35ccef0422d99ef9c73a8d8822a",
      "7b9bcecd717f4c23b693bceb62ef9eed",
      "bb3c7b05646140ba979af9c4817b8d26",
      "b1f395470dee459ab21eb651229d76cf",
      "d0cf64cc683343e4ad93ca299d9f13c3",
      "76a70408f8bd4871bc44167ba6b81a6f",
      "3a2f594a69d143c09f2390543d435453",
      "ed479d67a4b341619808d5a2eb2f7901",
      "3e155aaf93104a1e88d541ad905762e1",
      "7c109ec088e44c83ba2ab0883797d061",
      "a1a31198163c4b8d95c44c4683206199",
      "1cb85fbf718c4c018ab8e0332ba0156a",
      "cb1469250da6435ea57aadbd40abab4f",
      "929dc571cdc24bc58e596e79b4cea1a2",
      "305c3e20679440ffa24063a754235211",
      "c620bd12f12242e8a87ff58719b4ebc1",
      "5d48e05b0543495f90617a486bbf9e71",
      "e5d40c2473f0490e999c0a1eda56f5ea",
      "d8a870da0a8a48cfa39fbad50be0ebc0",
      "e4059fe44d8f423d80f990900c847a43",
      "a78be5397b6f4207b45ae0a375243a04",
      "6f16c224bfd94977a8f569475c7fe18b",
      "856242a694004876874d58ee0c387222",
      "e672b44c1b704f7d94a3e5d7fa0830bf",
      "295bceeeee0943af995f1dba5c188228",
      "4e9c08bf46304068874a51cbd77567e3",
      "21bcb62ba71c483eb70d08a30f029279",
      "fbb6be569ddd4af7b1a0ce7a7b174a34",
      "f16778f5588c4a8b8170229a27e7271e",
      "1320af3a5eea422ea9499363af354c7f",
      "abc05009ce554c0aa6721f00ff201599",
      "0f9a443ff4cb4309b4945e5435c53ffd",
      "0202319f364b4322b4592660fa99c2a4",
      "77dee3f599624017a1aa3fb6027f3d41",
      "d4cfca259254457aad148140e1a2a904",
      "953489c361ff4bb7815e53aa3b338419",
      "78e851fd6f184cdc99e0133cdf8259f4",
      "f101a158927e42da9d3b4763c9036c21",
      "a707d9f04ced417c987fc3ff01c1e08b",
      "6996e68476c547d4aa85ad363440db1c",
      "fe7ece4ebbae4a908594d028b3e49add",
      "a295e61d69a6433fa972e2d348465b8d",
      "43131ac31a58434fab0938a6d7ba00c3",
      "7144a824b3bf43a4bdea033d792d784d",
      "22fbdf888b5e4031abd21f3644df2c6e",
      "12bfbdaedf984306a04e6e3e90dd564a",
      "e2d883e4cd9b466eb5c717477a02304f",
      "ce1f61dfa21d4562942dbc6f32538c95",
      "076c56fc07ed452b94aa8c6d148e5e99",
      "2bd2058a82b844f3baf0cb78e3c7a53c",
      "2044ac7da0b143c8ba62a92cbe68b33d",
      "afb6f63aacdf408887b384c8b3c0c294",
      "9d2f8d597798453c82781388696b531f",
      "9ad1474d10fa47aaa1d32ed3f207fe05",
      "be762767540a4147b9cafee6d304dd71",
      "0e4e68bf42d5494da2dc04f94b47a2bc",
      "a0418d74573046428731ce0083b180f4",
      "3abcfca5d0f540c7941e1253c1ed5eac",
      "e18387ffdc8b4dd78bb4de2f1a694adf",
      "70aec5aa85f344fea98a4dc940114b46",
      "b98f7f3895834e83af43309529b87b76",
      "a39d429b67a048349cba79dc4d0b8dc0",
      "e1c84ab6619d47f693c4b10ed98c8317",
      "1b73d2d8b2eb4c2fb511341d7b3600f3",
      "02820cd1bc1a43ccae2aaae0f2986aa5",
      "908d63e5ee584aaeb73a544db6d59a49",
      "2377fadb33ce45e5a35cc49f6c422a13",
      "cc472a4ceacb493b83153981ee9a1259",
      "62e2be151c1046fe85edbae0fc730ef4",
      "81c258f8d31a45e291093e58214034e8",
      "2eb9a4479e66482fbc78150199eaa3ca",
      "d789470260654e4cb20208d26edd2dbd",
      "04b5ac4de9c24fbc977174477035f8a8",
      "8196f4ab3a6443bd9c601db51f770af1",
      "0fc72cb8bf9b41f2a83d17d819fb39d5",
      "24928647e7564f0bbfdb74254eebb804",
      "4642e048b75f47e6979b247f75396fd8",
      "6d450fa322ec450c97e2997121d2cef8",
      "60e8884db1be4757be8ff915183dce33",
      "d327351e95004afe9adc8cd0efae8bf4",
      "4636a6b43a094066a4a4ff5413203f15",
      "531bb0fd9fc14414afe8f2a3b57dea81",
      "eae0bdd71ce542268f7ea5a170b9473f",
      "599d65c3aca440629ddf96e6de2c6d56",
      "830b123503724ae788dc03a634104128",
      "91b8cc3a9d2244028dde9c5397236b99",
      "6fd3f977541b47daab47e307a75b4d33",
      "5f489bccbcfa4b6a80c854bffc12ea5d",
      "dc2272f46ab946bbbcc58d986fe5ed31",
      "a9ea1e76af0145439c55035eced1d127",
      "e26550bac07c4ef2a4161c25e967e0c4",
      "cf0f747c929543c49bc8ce84da67405d",
      "de00f00135ef4a4abf430b1fa770943c",
      "10c1476e50974fbba78c8847f1cfabfa",
      "1c14b08c3e034d6783abca03b0a48180",
      "306c5087fbb74a7db0dd0609ed7294c1",
      "f1be1317ad244b9ea58bd63a9d46abf8",
      "9e56a5222bf440ee9d430a6aa3b885be",
      "72f419fae3c94a9983a203124a69fd6d",
      "957bc3138f664ab68cb15dc938056c34",
      "f1d3cd96746a4a0f846a805f2304b74a",
      "ebadf82809cc48b1ab5a1f2b0ad895d3",
      "861dbfd1b7a24f2f9017b1dade23cafe",
      "0b494fb17ca148e79dfe26d9e5989377",
      "48ace4a3e283431c858ec8f4921c5d60",
      "d885eb367d6e402ea2bdd3a2552e2c45",
      "e3c0dd30fa504986b836d683f0cb1e34",
      "cb8738c9547147e4be4918af314911e9",
      "0ebff0cbdf0046349cedb24f8a70de0c",
      "ef6c9fadd7cf45f48eb2b19c47709bb1",
      "0704920d77334700b1ed1854709c21f0",
      "046a17c38ef947cca5d5a6b03c946d50",
      "faf794bb112443ada5251817fa2ed78b",
      "acc5e61791d54c7698a358b5fa14793b",
      "1c8f178acff84f798e63cf1c8d5d3597",
      "2e339550419341ac912d0e6a764bdfd8",
      "d56e6ff1e156465b9f7fd4d7ab81d302",
      "2130f98bf79c4fcdb9181b5ab99db5d9",
      "ad68ab13f1a9471ab5fb9bd2cd204983",
      "77dd811327024973af0609cf0d50640c",
      "c7379a2e17e14907b2d15548364455ec",
      "1fdf504815804e59baa8a5fc5b1ec99e",
      "6ed56d279be14583b199b3f5642432e1",
      "44c18eefc5a24adf8825eb6442911cff",
      "c545a2b24f324842a9f51e2d91d0002b",
      "1704941734254bd0a53a2610b7dbb4f4",
      "e9e3b414d0c5466c9d7b6af8f6530c1e",
      "4d68b7bfa87a4d5aa89e02d234b09c3d",
      "7e373a37b41549acabf357ffe3249ff4",
      "79b5ed2736e34a23853a460d548deb2b",
      "a57d4cf310444ae8af4581fb16a463a4",
      "a1e5d466afe5458ebacf08039ec2afd4",
      "82597bbd6c2c4a098531231967ed5638",
      "2d9252914e8943feb444f9117d953b4b",
      "b790d53c28e64251b50f6c4b2ed574a5",
      "fb3df691b05742ebbd7e35e803f15bd0",
      "5142b06f8d134a778f7523ec0c67428e",
      "4bc3ae64834f4dcc9346e3a547d49139",
      "5121b745afce4df58e60017a40b9a4f3",
      "01314d6f3c0b4e75b6671620e8632284",
      "2d21aad9ffe44f8e9d3944d9e5e82b90",
      "fd378b5bdcb84868ab537ae7731658c7",
      "7c7e45b385494a2284a8ef642329b28b",
      "7c023876b22244d48b57ed323befb058",
      "c7cd37750b024176a4eb78e0367ce1cf",
      "8d685203d8ce40d3922394b118ebd2a3",
      "a83f4c69f268436fa8991886b5388efb",
      "06c953d5e6224e61ad2c69486ea5221f",
      "c647930c41f54e858e5f2b9b5afeff95",
      "7b5826a2b5d348b6ac2ed2fb913dfd11",
      "c976b67abb744a99b6c8e2d110e0fac3",
      "b8d87137707e4682bd675083ec5e3e8f",
      "a5015ff069a54de891c735e12a4dbd49",
      "4213018c3c1c441483f55c66e7bf2eed",
      "cd698b80979f43e2b1c354328da3b4e6",
      "ccdce765ca6447218ee9384365921ef8",
      "5e2e498cd60c40b281c85ec6fac6d9c3",
      "1ae832b23ce44641a60b52331d1f446c"
     ]
    },
    "id": "QEWx-ElyVLS_",
    "outputId": "1165696c-59d3-45e5-9b8f-a52e24f6477a"
   },
   "outputs": [],
   "source": [
    "for epoch in range(epoch_num):\n",
    "  print(f'Training Epoch number {epoch}')\n",
    "\n",
    "  net.train() # set the model to training mode\n",
    "  running_loss = 0.0\n",
    "\n",
    "  for images, labels in tqdm(train_loader, desc='Training loop'):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad() # Clear old gradients\n",
    "\n",
    "    outputs = net(images)\n",
    "\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "  print(f'Loss: {running_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm_hx4Kdbc_g"
   },
   "source": [
    "Saving **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpNbKPeGYCRz"
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNaFz4SGbfp9"
   },
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MERtjnZWYvLm",
    "outputId": "58f88b75-9b52-4d39-fe37-ef2e4d3cfbce"
   },
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DVZa0f2ZBYL",
    "outputId": "9fd97650-bcfc-4415-833f-8d9c1e1c8845"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "total = correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_loader:\n",
    "    images, labels = data\n",
    "\n",
    "    outputs = model(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy is: {accuracy} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoJK14vMblp6"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_ShIvJQbm7y"
   },
   "outputs": [],
   "source": [
    "new_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd-furJXfQoP"
   },
   "outputs": [],
   "source": [
    "def loadImage(image_path):\n",
    "  img = Image.open(image_path)\n",
    "  img = new_transform(img)\n",
    "  img = img.unsqueeze(0)  # Adds a batch dimension so shapes are compatible\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQqFbnrIftE9"
   },
   "outputs": [],
   "source": [
    "img_paths = ['plain.jpg']\n",
    "images = [loadImage(img_path) for img_path in img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cU1StxZvf_TV",
    "outputId": "5229ee35-a5ce-4d28-b4e9-f851475c026a"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for img in images:\n",
    "    outputs = model(img)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print(f'Prediction: {class_names[predicted.item()]}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
